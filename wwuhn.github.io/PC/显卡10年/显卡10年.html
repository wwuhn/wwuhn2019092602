<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
 
<title>显卡10年</title>

 <style type="text/css">
#tbrowser a:link，.container a:visited{
font-size:18px;
text-decoration:none;
}
.container{
font-size:1.2em;
margin:auto;
font-family:"mv boli","宋体";
width:75.29%;
line-height:1.4em;
}
P{
margin:8px;
text-indent:2em;
}
.uls{
color:#CC6666;
font-weight:bold;
}
.uls>ol{
list-style:none;
font-weight:normal;
list-style:lower-latin;
color:#000000;
line-height:1.3em;

}
h2{
font-size:20px;
font-weight:bold;
margin-top:5px;
margin-bottom:-15px;
text-indent:0em;
}
img{
margin-right:5px;
}
.fc{
color:red;
}

a:link {
	text-decoration: none;
}


a:visited {
	text-decoration: none;
}
a:hover {
	text-decoration: none;
}
a:active {
	text-decoration: none;
}
table{
border:0px solid red;
}
</style>
</head>

<body>
<div class="container">
  <p align="left">显卡产业十年发展回顾 </p>
<p>
  &nbsp;<img width="500" height="406" src="显卡10年_clip_image001.jpg"  /></p>
<p>
    又是一个平凡的晚上，翻开最底层的抽屉，突然发现那个既熟悉又陌生的身影，它身上那个仍然清晰可见、曾经让无数电脑游戏迷疯狂的烙印，让人觉得十年时光的飞逝实在来得太快了，在不经意间你已经将近离开我们十年了——唯一一个在玩家心中能上升至精神层面的显示芯片公司，3Dfx。从普通的游戏玩家到普通硬件玩家，从硬件玩家到论坛版主，从论坛版主到显卡编辑，看着手中这块Voodoo，笔者感叹自己已经见证了这个显卡产业成长整整10年的时间，因此钩起了无数回忆。随后，笔者决定和读者分享这些，从了无生气硬件中引发出来的带感情的回忆。 </p>
<p>
  <strong>1999</strong><strong>年：</strong> </p>
<p>
  <img width="500" height="332" src="显卡10年_clip_image002.jpg"  /> </p>
<p>
    VOODOO3</p>
<p>
    3Dfx与NVIDIA命运的交替正是从这一年开始的，TNT2系列尽管实际游戏性能并不及VOODOO3，但凭借AGP 4X总线（DOOVOO3开始还死抱着PCI总线不放）、更新的工艺、更高的频率等表面规格，为NVIDIA在独显市场上开始争夺大量的市场份额，其实其中最重要的是NVIDIA灵活的细分产品线规划，光一款TNT2系列就有共5个型号和不同显存容量的版本，在灵活的市场策略下不少显卡品牌纷纷投入NVIDIA怀抱。而反观当时的VOODOO3只有3款产品（不包括3500），在合作伙伴慢慢疏远的情况下3Dfx毅然决定收购显卡品牌STB，采取对外统一供货的策略，同时计划打入出货量日渐可观的OEM市场，但后来事实证明这是致命的错误决定。 </p>
<p>
  <img width="500" height="369" src="显卡10年_clip_image003.jpg"  /> </p>
<p>
    VOODOO3 3500TV</p>
<p>
    3Dfx为何如此让人着迷，其中很大一个原因是其创造了不少新颖的行业指标，象现在为人所熟知的SLI多卡互联技术、多核心同卡和上图中的多媒体功能显卡始祖——VOODOO3  3500TV，这种概念后来被ATI搬到著名的ALL IN  WONDER系列显卡上去。 </p>
<p>
  <img width="500" height="356" src="显卡10年_clip_image004.jpg"  /> </p>
<p>
    NVIDIA Geforce256</p>
<p>
    如果说TNT2是一款让3Dfx失去信心的产品，那么Geforce256就是一款让3Dfx绝望的产品，在1999年的9月1日，NVIDIA发布了全球首款支持硬体T&amp;L的显示核心，并赐与一个必须记入硬件史载的名字——GPU。NVIDIA率先把T&amp;L（Transforming以及Lighting，光影转换）归到显示核心的工作中，同时加入动态光照引擎、四路像素渲染管线以及DirectX7支持等强劲硬性规格。T&amp;L原先由CPU负责，或者由另一个独立处理机处理 (例如一些旧式工作站显视卡)，尽管强劲的3dfx Voodoo和Rendition  Verite显视核心整合了几何 (三角形) 建构, 但硬件T&amp;L仍是一大进步，原因是显示核心从此CPU接管了大量工作。 </p>
<p>
    GeForce256凭著它的功能和速度，让NVIDIA的电脑图形工业霸主地位更坚固。NVIDIA的成功，使3dfx, Matrox, 和S3 Graphics都黯然失色。就在GeForce256发布后的几个月，竞争对手S3亦发布了 Savage 2000。此产品也内建了硬体T&amp;L，价格比GeForce256便宜。但是有缺陷的驱动使T&amp;L不能正常运作，而S3亦不打算对此修正。在此之后就只剩下ATi的Radeon显卡能有实力孤军作战。 </p>
<p>
  <strong>2000</strong><strong>年：</strong> </p>
<p>
  <img width="500" height="373" src="显卡10年_clip_image005.jpg"  /> </p>
<p>
  <strong>销售量的神话——Geforce2 MX </strong> </p>
<p>
    真正为NVIDIA夺下半壁江山的并不是Geforce256，而是其所展开出来的细分产品线。2000年6月，NVIDIA向低端市场推出的基于NV11核心的GeForce2 MX系列显卡，这系列显卡曾经在一段时间内统治了整个主流市场，即使到GeForce3显卡推出，仍在市场热卖。同时GeForce2 MX系列也为NVIDIA与ATI争夺市场的时候增加了筹码。  
  
    <table  cellspacing="0" cellpadding="0">
      <tr>
        <td> 
            <img width="500" height="368" src="显卡10年_clip_image006.jpg"  /> </td>
   
        <td> <img width="500" height="374" src="显卡10年_clip_image007.jpg"  />  </td>
      </tr>
    </table>
   
  <p >经典的直升飞机test，估计是不少人第一眼看见3Dmark的场景 </p>
<p>
    2000年Futuremark（其实当时还没改名，还叫Madonion）发布了3DMark2000测试程序，基于的DirectX版本更新到7.0，测试的硬件需要支持T&amp;L，并且加入了32bit色彩渲染技术。此时的3DMark 2000选择追逐最新技术，而非照顾主流游戏的实际应用情况。3Dmark  2000一经推出顿时成为了3D性能的唯一通用指标，并延续至今。基于DirectX3D API接口的3Dmark的兴起，一定程度上预示着GlIDE API接口的没落。 </p>
<p>
  <img width="500" height="374" src="显卡10年_clip_image008.jpg"  /> </p>
<p>
    如果没有记错的话，众所周知的射击竞技游戏CS是从2000年开始流行起来的，那时候开始无论任何网吧，任何时间都能听到这款游戏传出来的声音。尽管这款游戏对显卡要求并不高，但当时来说一款集成显卡仍然是不足以应付CS的流畅需要的，因此CS创造了一个独立显卡热销的时期，同时，因为ATI Radeon显卡在CS里面问题多多，所以可以说这个独立显卡的热销期最大的受益者是NVIDIA，Geforce2 MX400几乎是当时玩CS的标准配备显卡。 </p>
<p>
  <img width="500" height="400" src="显卡10年_clip_image009.jpg"  /> </p>
<p>
  <strong>显示领域一代霸主魂断千禧年——3DFX被对手收购</strong> </p>
<p>
    2000年在显卡发展史上记录最大的事件莫过于3Dfx被直接竞争对手NVIDIA收购。由于VOODOO4的性能未如预期，而双核心的VOODOO 5的价格又居高不下，促使3Dfx的支持率急剧下降。面对这个不利局面，3Dfx将所有希望放在了Microsoft身上，以1.8亿美元的价格准备收购Gigapixel公司、希望获得Microsoft将要推出的X-BOX的图形芯片订单。但是Microsoft最终却选择了NVIDIA，3Dfx的股票再次下措，而且计划中RamPage也被无限期地搁置！毫无生机3DFX最终被NVIDIA仅以一亿一千二百万美元的低价收购了。从此，这个极具传奇色彩的显示芯片厂商最终仅仅留存在众多硬件发烧友的脑海中！ </p>
<p>
  <img width="446" height="280" src="显卡10年_clip_image010.jpg"  /></p>
<p>
    Rampage（上）和Voodoo5 6000（下） </p>
<p>
    这次收购后，造就了世界上最具收藏价值的显卡——拥有4个核心的怪物显卡VOODOO5 6000，也创造了一个显卡史上解不开的秘密——Rampage的性能之迷。从很多不确实消息中所透露出来的规格，可以估算出Rampage的性能将会在Geforce3之上。但肯定没人知道Rampage的真实性能，因为这星球上只存在着几块不能正常工作的Rampage。Rampage给3Dfx  FANS们留下了一个不能终结的梦，也许在这世界上，得不到的东西才是最珍贵的吧。 </p>
<p>
  <strong>2001</strong><strong>年：</strong> </p>
<p>
  <img width="500" height="306" src="显卡10年_clip_image011.jpg"  /> </p>
<p>
  <strong>图形核心产业进入DirectX8——Geforce3</strong> </p>
<p>
  <strong>　　</strong>2001年显卡产业迈向了DirectX8时代，在收购3Dfx3个月后，NVIDIA又推出了GeForce3，采用全新的核心架构，支持可编程的vertex/pixel shader——这是衡量DirectX8显卡的重要指标。GeForce3是首款支持Shader Model 1.1的显卡，采用了更有效的显存管理方案，多重取样抗锯齿性能是当时市面上最强的。 </p>

  <div >
    <table  cellspacing="0" cellpadding="0">
      <tr><td>
  <img width="398" height="146" src="显卡10年_clip_image012.jpg"  /> </td>
        <td>
            <img width="500" height="374" src="显卡10年_clip_image013.jpg"  /> </td>
      </tr>
    </table>
  </div>
  <p >3Dmark2001推出初期，仅有少数手上有Geforce3的骨灰玩家能观看测试全程 </p>
<p>
    标志着进入DirectX8时代的还有另一个大事件，正是新版本3Dmark的发布。2001年3月14日Futuremark发布了基于DirectX 8.0技术的3DMark2001，引入了“像素渲染”概念，支持Vertex Shader与Pixel Shader特效指令，照顾兼容了DirectX 7.0显卡，能最真实地反应显卡的性能。是历史上最引人瞩目的一代3DMark产品。随后更新到3DMark2001SE，支持DirectX 8.1技术，Pixel Shader需要1.4版本。3Dmark2001的更新让当时所有的Geforce2系列显卡都趴下了，因为不支持DirectX8的话，则不能启动最后Direct8X场景的测试，导致丢失了该场景的得分，Geforce3成为当时唯一能跑完全程测试的显卡，不过要体验全程测试是需要代价的，当时3000元的人民币真不是谁都能掏出来的。 </p>
  <div >
    <table  cellspacing="0" cellpadding="0">
      <tr>
        <td></p>
<p>
            <img width="500" height="350" src="显卡10年_clip_image014.jpg"  /> </td>
      </tr>
    </table>
  </div>
  <p align="left"><strong>Radeon8500</strong><strong>让ATI首尝胜果——</strong> </p>
<p>
    尽管Geforce3的强大是不需质疑的，但笔者并不认为Geforce3是一款经典的产品，原因是它的辉煌完全被对手Radeon8500掩盖，即使是NVIDIA后来变阵祭出频率更高的Geforce3 TI500，仍然未能在3Dmark2001中取胜（但其实Radeon8500的游戏性并不怎样），与99年3Dfx相似的命运在嘲笑着NVIDIA；可以说Radeon8500让ATI在桌面市场上初次尝到胜利的美酒，也标志着ATI和NVIDIA双雄并起的局面正式开启——直到今天，哥俩还争的好不热闹。 </p>
<p>
  <strong>2002</strong><strong>年：</strong> </p>
<p>
  <img width="500" height="334" src="显卡10年_clip_image015.jpg"  /> </p>
<p>
  <strong>DirectX8</strong><strong>时代的绝对强者——Geforce4 Ti</strong> </p>
<p>
    不过，就在Radeon8500上市后的2个月，NVIDIA就发布了新的Geforce4系列，很负责地说Geforce4 Ti系列代表着DirectX8时代的最高性能，当时只有Geforce4 Ti才能轻松使3Dmark2001分数达到1万以上，同时也拥有着Geforce4 Ti4200这个历史上罕见的性价比之王，笔者当年就拥有一个Athlon  XP 1800+和Geforce4 Ti4200的平台，跑3Dmark2001那种前所未有的流畅感到现在还历历在目。 </p>
<p><table><tr><td>
  <img width="500" height="118" src="显卡10年_clip_image016.jpg"  /> </td><td>
  <img width="500" height="342" src="显卡10年_clip_image017.jpg"  /> </td></tr></table></p>
<p>
  <strong>SIS</strong><strong>的Xabre，业界第一款AGP 8X显卡——</strong> </p>
<p>
    AGP1X总线的66Mhz工作频率使其带宽从PCI总线的133M/S整倍增加到266M/S，AGP的诞生意味着显卡终于可以从PCI总线中解放出来，有一条独立的总线为之服务。到了2002年，首先进入AGP8X时代的不是NVIDIA，也不是ATI，而是著名芯片组厂商SIS携Xabre400带领业界各巨头走进了AGP8X时代，AGP8X是AGP历史最后一个规范，带宽从最初AGP1X的266M/s提高到2.1G/S，以满足飞速发展的图形显示芯片需要。 </p>
<p>
  <table><tr><td><img width="500" height="387" src="显卡10年_clip_image018.jpg"  /> </td><td>
  <img width="500" height="337" src="显卡10年_clip_image019.jpg"  /> </td></tr></table></p>
<p>
  <strong>MATROX</strong><strong>的幻日，首款512Bit核心、256Bit显存位宽的显卡——</strong> </p>
<p>
    除了接口标准提升外，核心和显存位宽的标准也在2002年被刷新，刷新“纪录”的选手同样不是NVIDIA和ATI，而是沉寂多时的MATROX。MATROX在2002发布了振奋人心的Parhelia显卡，所有人都认为MATROX终于高调回归3D显卡市场了。从规格上看，Parhelia让MATROX看到了攀上3D巅峰的希望：业界首款256-bit显存位宽显卡，首款512-bit位宽核心显卡（至今无人超过），220MHz核心频率，此外该显卡支持'Surround Gaming' 模式，可以让游戏玩家在三台显示器上玩游戏，该显卡甚至标榜支持DirectX9。不过，MATROX后来承认Parhelia的顶点shader单元与DirectX9标准不兼容，所以无法支持DirectX9，尽管之前Matrox声称Parhelia是首款支持DirectX9特效的显卡。更糟糕的是，Parhelia售价高达400美元，而其3D性能和游戏兼容性实在不敢恭为，所以最终它没有形成异军突起之势。 </p>
<p>
  <img width="500" height="344" src="显卡10年_clip_image020.jpg"  /> </p>
<p>
  <strong>红色军团大反扑——Radeon9700Pro发布</strong> </p>
<p>
    一年前，ATI用Radeon8500告诉业界他们有着和NVIDIA叫板的技术能力，一年后，ATI用Radeon9700Pro向业界宣布有着超越NVIDIA的技术能力。ATI对未来豪赌了一把，将宝完全押在了微软的DirectX9上。Radeon9700Pro的R300核心采用了新的工艺使得晶体管数目加倍却同时能拥有极高的频率，其硬件架构最特别的地方在于像素渲染管线与TMU的比例为1：1，8条渲染管线形成了8x1的先进硬件架构。R300高级交叉内存控制器带来了256位的总线宽度，提供了接近20G/s的内存带宽。4个高级定点着色单元每个单元能够同时进行一次矢量和一次标量运算。更高级的SSAA和MSAA抗锯齿模式引入，最高16X的各向异性过滤性能，完整支持DirectX9的硬件规范，具有超长指令集和超长常数寄存器，DirectX9规定的MRT和24位浮点精度的完整支持等等。R300成为了当时性能最强技术最先进的显示芯片。在当时NVIDIA最高端的Geforce4 Ti4800的比较中，R300性能超出了15-20%，而AA与AF打开后，优势更是扩大到了40%-100%。NVIDIA被打了个措手不及，即使是后来推出的GeforceFX5800因为错误估计了DirectX9游戏大潮到来的时机，仍然采用了保守的4x2架构，尽管引入了相当多的新特性，却完全没有反击的机会。 </p>
<p>
  <img width="500" height="341" src="显卡10年_clip_image021.jpg"  /> </p>
<p>
  <strong>牵起全球改卡风暴——Radeon9500</strong> </p>
<p>
    不得不提的是伴随着Radeon9700Pro的风光，Radeon9500也在全球刮起了一股改造风暴，硬件发烧友发现不少Radeon9500可通过软硬各种方式把屏蔽的四条象素渲染管线打开，摇身一变成为价值翻倍的Radeon9700Pro，一时间全线基于R300的显卡都成为玩家追捧的产品。 </p>
<p>
  <strong>2003</strong><strong>年：</strong> </p>
<p>
 <table><tr><td> <img width="500" height="202" src="显卡10年_clip_image022.jpg"  /></td><td>
  <img width="500" height="374" src="显卡10年_clip_image023.jpg"  /> </td></tr></table></p>
<p>
    让人又爱又恨的DirectX9画面 </p>
<p>
    Futuremark在2003年2月12日发布这个革命性的DirectX 9.0测试软件，继续发扬Pixel Shader与Vertex Shader的重要性，Shade版本已经更新到3.0。和3Dmark2001se一样，3Dmark03同样是一款让旧卡崩溃的测试软件，因为在DirectX9到来之际，市场上仍然充斥着大量DirectX7-8的显卡，主流的Geforce4 MX440仅能在3Dmark03里面完成一项测试，3位数的得分当时实在让人哭笑不得。另外由于初代的DirectX9主流显卡的实际游戏性能都不尽如意，所以还有不少死抱着Geforce4  Ti不放的理性支持者，反正当时硬件论坛上两大观点互攻的场面很是热闹。 </p>
<p>
  <img width="500" height="290" src="显卡10年_clip_image024.jpg"  /> </p>
<p>
  <strong>NVIDIA</strong><strong>最大的黑历史——GeforceFX5800 Ultra </strong> </p>
<p>
    由于关系到性能表现、新的制造工艺、新的显存方案等等众多问题，NVIDIA的新旗舰GeforceFX5800系列拖延到2003年的2月出现。GeForce  FX 5800 Ultra采用了当时尚未成熟的0.13微米工艺，集成了夸张的1.25亿个晶体管，史无前例地配备8条渲染管线，频率方面提升至500MHz，架构方面继续采用了4x2架构。但由于核心频率上以及晶体管数量的大幅增加，使得GeForce FX 5800 Ultra的发热量以及功耗也成倍增加，更加令人失望的是，GeForce  FX 5800 Ultra仅是一款128bit位宽的产品，为了提高显存带宽，NVIDIA为其提供了尚不成熟的GDDR2显存，仅8枚GDDR2显存芯片需要消耗28W功耗，为此NVIDIA不得已为其打造了专用的“FX Flow”散热器，而且要霸占一条PCI槽，成为产品的诟病。 </p>
<p>
  <strong>2004</strong><strong>年：</strong> </p>
  <div >
    <table  cellspacing="0" cellpadding="0">
      <tr>
        <td></p>
<p>
            <img width="500" height="350" src="显卡10年_clip_image025.jpg"  /> </td>
      </tr>
    </table>
  </div>
  <p align="left"><strong>DirectX9</strong><strong>时代的销量神话——Radeon9550</strong> </p>
<p>
    尽管是一款主流级的产品，但Radeon9550的确值得记入显卡发展史载中去。Radeon9550是在DirectX9时代特别存在的显卡，它继承了优良的R3XX架构血统，拥有非一般的超频能力，使其成为ATI历史上出货量最大的型号，无论是3Dmark还是真实游戏中，Radeon9550都创造过超频后分数翻倍的奇迹！超频后性能甚至能达到当时千元级GeforceFX5900XT的水平，所以当时市面上充斥着各种各样超频版的Radeon9550，这些Radeon9550军团几乎占据了整个千元以内市场。 </p>
<p>
  <img width="500" height="280" src="显卡10年_clip_image026.jpg"  /> </p>
<p>
  <strong>AGP</strong><strong>时代终极武器——Geforce6800 Ultra</strong> </p>
<p>
    Geforce6800Ultra是AGP时代无可非议的性能之王，它是一款不计成本，誓要夺回性能王座的产品。在规格上，Geforce6800Ultra的NV40核心采用了恐怖的2.22亿个晶体管（基本上是0.13微米工艺的极限，所以NVIDIA转向与IBM合作引入冗余电路技术提高良品率），史无前例的16条像素流水线，这些都接近ATI R300核心的2倍数量。总体而言，GeForce6系列将Shader部分作为重点，其整个体系架构可以分为三大块，分别是Vertex Shader（顶点着色器）、Pixel Shader（像素着色器）和ROP（像素结果输出处理器）。除此之外，NV40还引入了第三代的CineFX 3.0引擎，完全符合DriectX 9.0C的Vertex Shader3.0和Piexl Shader3.0规范，这是当时ATI所不支持的范畴。 </p>
<p>
  <strong><img width="500" height="366" src="显卡10年_clip_image027.jpg"  /></strong><strong> </strong></p>
<p>
  <strong>造就第二次难分高下的机会——RadeonX800XT</strong> </p>
<p>
  <strong>　　</strong>RadeonX800系列与Geforce6800系列发布的日期相当接近，不过与NVIDIA的全面革新架构不同，RadeonX800系列的R420核心架构上和R300非常类似，基本就等于两倍的R300像素渲染资源，16条像素渲染管线和16个光栅处理器，增加了PS指令的长度，加强了F-Buffer的管理，最大亮点估计在于提前支持PCI-E 16X标准。其他特色包括名为3Dc的Normal Mapping压缩技术，使用RGB或者RGBA格式将法线贴图压缩到4：1的程度而不会造成大量画面信息的丢失。ATI坚称还不到更新到SM3.0的时机，所以R420所支持的只是SM2.0+的技术范畴，这点引至后来成为NVIDIA攻击的缺口。 </p>
<p>
  <img width="456" height="230" src="显卡10年_clip_image028.gif" alt="http://www.techcn.com.cn/uploads/200910/1254625965BL2vwYtr.gif" /> </p>
<p>
  <strong>总线变化，多卡互联时代来临——PCI-Express</strong> </p>
<p>
    显示核心的处理吞吐量每代产品都呈成倍增长，到DirectX9的中期AGP总线已经开始满足不了显卡的需要。业界老大INTEL早早就意识到这问题，早在2002年开始制订了PCI-E总线的标准。PCI Express采用了流行的点对点串行连接，比起PCI以及更早期的计算机总线的共享并行架构，每个设备都有自己的专用连接，不需要向整个总线请求带宽，而且可以把数据传输率提高到一个很高的频率，达到普通PCI总线所不能提供的高带宽。相比传统PCI总线在单一时间周期内只能实现单向传输，PCI Express的双单工连接能提供更高的传输速率和质量，它们之间的差异跟半双工和全双工类似。因为PCI总线总是共享并行架构标准，所以PCI-E显卡时代的到来也意味着多卡互联的回归。 </p>
<p>
  <img width="450" height="182" src="显卡10年_clip_image029.jpg"  /> </p>
<p>
    3DMark05依然沿用DirectX 9.0版本，相比3DMark2003基本没有分别，不过其全面改用微软提供的HLSL进行渲染引擎的编写，以让引擎自动适应测试显卡的硬件架构，此时全面转向Windows 2000/XP测试平台。 </p>
  <div >
    <table  cellspacing="0" cellpadding="0">
      <tr>
        <td></p>
<p>
            <img width="500" height="347" src="显卡10年_clip_image030.jpg"  /> </td><td ><img width="500" height="344" src="显卡10年_clip_image031.jpg"  /> </td>
      </tr>
    </table>
  </div>
  
<p>
    AGP/PCI-E平台双赢的Geforce6600GT</p>
<p>
    尽管在命名上为3Dmark05，但其实3Dmark05是诞生于2004年的第四季度初的，记得当时正值AGP与PCI-E接口交替之际，NVIDIA与ATI在这过渡期中抱着截然不同的态度：NVIDIA致力开发其BR01桥接芯片，让原生AGP的芯片可以桥接到PCI-E接口，让原生PCI-E的芯片可桥接到AGP接口；而ATI则鼓吹“有路何需搭桥”的口号，钟情于原生芯片原则。最后的结果告诉我们，过渡时期必须有着灵活并健全的策略，NVIDIA在这过渡期中略胜了一筹，其中在AGP/PCI-E平台中获得双赢的产品，正是经典的Geforce6600系列。 </p>
<p>
  <strong>2005</strong><strong>年：</strong> </p>
<p>
<table>
<tr>
<td>
  <img alt="left" width="469" height="480" src="显卡10年_clip_image032.jpg"  />  </td><td>
  <img width="500" height="332" src="显卡10年_clip_image033.jpg"  /> </td></tr></table></p>
<p>
    迟来的多卡互联——Corss Fire</p>
<p>
    自PCI-Express普及后，多卡互联技术就迅速成为热门话题，因为要在现有晶片工艺下让性能迅速增长，多GPU互联是最直接有效的方法，拥有3Dfx SLI技术的NVIDIA自然在这方面先行一步，全球3Dmark排行榜上ATI的产品很快就消失了。在这一年ATI终于拿出了自家的双卡互联技术——Cross Fire。CrossFire技术是为了对付NVIDIA的SLI技术而推出的，也就是所谓的“交叉火力”简称“交火”。与NVIDIA的SLI技术类似，实现CrossFire技术也需要两块显卡，而且两块显卡之间也需要连接(只是在机箱外部而非内部罢了)。但是CrossFire与SLI也有不同，首先主显卡必须是CrossFire版的，也就是说主显卡必须要有图象合成器，而副显卡则不需要；其次，CrossFire技术支持采用不同显示芯片(包括不同数量的渲染管线和核心/显存频率)的显卡，只是较高档显卡多出的渲染管线会被自动关闭而且频率也可能会自动降低到性能较低显卡的水平。 </p>
<p>
    在渲染模式方面，CrossFire除了具有SLI的分割帧渲染模式和交替帧渲染模式之外，还支持方块分离渲染模式(SuperTiling)和超级全屏抗锯齿渲染模式(Super AA)。方块分离渲染模式下是把画面分割成32X32像素方块，类似于国际象棋棋盘方格，其中一半由主显卡负责运算渲染，另一半由副显卡负责处理，然后根据实际的显示结果，让双显卡同时逐格渲染处理，这样系统可以更有效的配平两块显卡的工作任务。 </p>
  <div >
    <table  cellspacing="0" cellpadding="0">
      <tr>
        <td></p>
<p>
            <img width="500" height="502" src="显卡10年_clip_image034.jpg"  /> </td>
      </tr>
    </table>
  </div>
  <p align="left">　　到了DirectX9末期的这个时间点上有一战是不得不说的，就是NVIDIA在Geforce7800GTX上，上演的一幕“螳螂捕蝉，黄雀在后”战术。ATI经历漫长的开发时间后（R520冒险采用新的90nm工艺制造），终于拿出了频率超高的RadeonX1800XT。RadeonX1800XT在3Dmark05中轻松破万，横扫当时的目标Geforce7800GTX 256M。正当ATI以为能安座最强性能之际，NVIDIA在RadeonX1800XT发布后不到一个月的时间里，发布了频率更高的Geforce7800GTX  512M。原来NVIDIA是有心在Geforce7800GTX  256M上隐瞒实力，终于在初代原生PCI-E显卡上，性能之争以NVIDIA胜出告终。 </p>
<p>
      <img width="500" height="271" src="显卡10年_clip_image035.jpg"  /> </p>
<p>
      <strong>DirectX9</strong><strong>的末代王者——3:1黄金架构</strong> </p>
<p>
    经过两代产品失利后，ATI仍然没有放弃DirectX9时代，在2005-06年调整出新的架构，ATI把其称为3:1黄金架构。面对像素处理的大幅度增加，两家最大的图形公司NVIDIA和ATI出现了分歧，对于总体问题上的认识，两家公司的态度是一样的，但是NVIDIA坚持将“像素处理器/纹理处理器”的比例约束在2：1的比例上，但是ATI的做法更为激进，在最新的产品中，ATI始终提供“像素处理器/纹理处理器”高比例，也就是3：1的比例，从游戏的发展趋势来看，这个比例更为贴切。除了这个理由，ATI的工程师还有另一个理由来说明这种比例的优势，那就是每个频率可以获取的带宽比例在下降（R300时是3:4、R580是7:8），纹理处理的消耗量其实是在增加的，这点是确定的（分辨率的提升、纹理的精细化），但这并不是纹理单元增加的理由，更合理的搭配才是最正确的选择，一味的增加纹理处理单元只会增加对带宽的占用，从而成为反面因素。 </p>
<p>
  <strong>2006</strong><strong>年：</strong> </p>
<p>
  <img width="500" height="332" src="显卡10年_clip_image036.jpg"  /> </p>
<p>
  <strong>双A另辟奇径——AMD并购ATI</strong> </p>
<p>
    尽管RadeonX1900系列性能强悍，但经过两代产品的失利，主要利润的中低端市场被对手全面夺取，ATI陷入了前所未有的困境。ATI股价大跌，长期在15美元左右徘徊。在ATI与Intel的合作意向谈崩了之后，AMD向ATI伸出了双手。2006年7月24日，AMD正式宣布以总值54亿美元的现金与股票并购ATI，10月25日，AMD宣布，对ATI的并购已经完成，ATI作为一个独立的品牌已经成为了历史。 </p>
<p>
  <img width="500" height="212" src="显卡10年_clip_image037.jpg"  /> </p>
<p>
  <img width="500" height="329" src="显卡10年_clip_image038.jpg"  /> </p>
<p>
  <strong>新时代游戏体验的开端——物理加速卡小试牛刀</strong> </p>
<p>
  <strong>　　</strong>PhysX是原AGEIA公司开发的一套物理运算引擎,主要竞争对手是Havok。同Havok一样，Physx也可运用在Xbox360，Playstation3，PC，Mac等多种平台之上。Physx的另外一个优势是可以运用独立的浮点处理器（包括独立的物理加速卡和GPU）经行更为复杂的运算效果，同时减轻CPU的计算负担。AGEIA曾为&lt;虚幻竞技场3&gt;开发几张加入物理卡计算的MOD地图，必须要需要AGEIA的物理卡才能进入游戏，物理效果非常出色，同时AGEIA也自己开发一个免费的名为&lt;好战者&gt;的物理加速游戏来推动自己的物理加速卡销售。AGEIA声称，PhysX将会使设计师在开发游戏的过程中使用复杂的物理效果而不需要像以往那样耗费漫长的时间开发一套新的物理引擎，而且使用了物理引擎还会大量消耗CPU资源使一些配置较低的电脑无法流畅运行。 </p>
<p>
  <table><tr><td><img width="332" height="287" src="显卡10年_clip_image039.jpg"  /> </td><td>
  <img width="500" height="374" src="显卡10年_clip_image040.jpg"  /> </td></tr></table></p>
<p>
    含HDR的北极场景是3Dmark06唯一视角上的新元素 </p>
<p>
    相信作为一个工龄超过2年的显卡评测编辑来说，都会比较讨厌3Dmark06这个测试，首先其毫无新意的画面让经常对着3Dmark的编辑产生了严重的审美疲劳，其次漫长的测试时间也大大延长了整个评测的时间。但偏偏3Dmark06是到目前为止生命周期最长的一款3DMark，到目前为止仍然流行。 </p>
<p>
  <img width="500" height="334" src="显卡10年_clip_image041.jpg"  /> </p>
<p>
  <strong>SLI</strong><strong>的首次爆发——Geforce7950GX2</strong> </p>
<p>
    NVIDIA在2006年5月发布了当时世界上最快的“单卡”Geforce 7950 GX2。Geforce 7950 GX2显卡包含两个Geforce7900GTX GPU，核心频率为500MHz，每个核心512MB GDDR3 1.2GHz的本地内存配置。该卡设计极为精良，基于SLI技术但可以在非SLI主板上正常使用，还能够使用两块Geforce 7950 GX2在支持SLI的主板上实现四核并联的Quad SLI技术，搭建远超竞争对手的超级平台。 </p>
<p>
  <img width="500" height="352" src="显卡10年_clip_image042.jpg"  /> </p>
<p>
  <strong>历史上寿命最长的旗舰——Geforce8800系列</strong> </p>
<p>
    06年底，NVIDIA发布了地球上第一款DirectX10显卡：Geforce8800系列。Geforce8800系列核心G80首次采用了DirectX10推荐的全标量统一渲染流处理器架构，128个流处理器＋384Bit显存位款的Geforce8800GTX/Ultra即使到今天性能仍然毫不落后。ATI在随后半年推出的首款DirectX10旗舰显卡RadeonHD2900XT在G80面前根本不值一提，在没有对手的情况下，G80稳坐最强性能宝座直至GT200出现，历时1年半以上，是历史上最长寿的旗舰产品。 </p>
<p>
  <strong>2007</strong><strong>年：</strong> </p>
<p>
  <img width="500" height="283" src="显卡10年_clip_image043.jpg"  /> </p>
<p>
  <strong>多媒体性能之战展开——高清规格的较劲</strong> </p>
<p>
  <strong>　　</strong>尽管在3D游戏性能方面不尽人意，但令人欣喜的是HD2000附带的AvivoHD硬解码技术效果要比Geforce8系列好。相比之下，HD2000还有一定的竞争力。ATI在RV630/610上都加入了Avivo  HD视频回放技术，其中的UVD通用视频解码引擎，能同时实现H.264和VC-1编码视频的硬件级解码，继续丰富图形加速显卡在视频回放方面的能力，进一步释放高清晰视频播放过程时对CPU运算能力的依赖。ATi强大的高清回放能力也成为ATi的Radeon HD 2400系列在入门市场上的重大注码。 </p>
<p>
  <img width="500" height="283" src="显卡10年_clip_image044.jpg"  /> </p>
<p>
    G7X和G80 GPU的PureVideo  HD特性依靠内部的VP（VideoProcessor）提供，在对高清视频进行解码时，能够完成除了Bitstream处理和InverseTransform之外的其它操作，包括对CPU能力要求不低的De-Blocking操作。但以H,264编码的高码率影片播放时，即使CPU被PureVideo HD从De-Blocking解放出来，Bitstream处理仍旧给CPU沉重的压力。 </p>
<p>
    G84 GPU在内部设计上大大增强了视频解码逻辑，除了VP版本更新并加强了性能之外，还新增了针对H.264解码的BSP（Bitstream  Processor）引擎，解决原来G7X和G80 GPU的PureVideo HD仍需CPU进行Bitstream处理的问题，彻底接手高清视频解码的所有工作。以G84GPU为核心的Geforce 8600系列显卡，现在能够基本不需CPU计算能力的支持，就能流畅播放高码率H.264压缩格式的高清视频，BSP支持CABAC/CAVLC两种方式的Bitstream处理，即使使用的是低速CPU，CPU占用率也可以保持在40%以下，系统响应度和播放顺畅度都能够保证。 </p>
<p>
  <img width="440" height="222" src="显卡10年_clip_image045.jpg"  /> </p>
<p>
    PCI-E 2.0来临 </p>
<p>
    随着Geforce8800GT发布，NVIDIA也率先进入了PCI-E 2.0时代， 在外观上PCI-Express 1.0与2.0并无区别，PCI Express2.0是向下兼容的，所以无须担心PCI-E 2.0显卡不能用在PCI-E 1.0的主板上。以下是负责拟定PCI Express总线标准的PCI-SIG放出关于PCI Express 2.0改进的特性： </p>
<p>
    1\每条串行线路的数据传输率从2.5Gbps翻番至5Gbps，每个PCI-E通道传输率提升至500MB/s（比1.0提速一倍）。</p>
<p>
    2\更好地支持未来高端显卡，即使功耗达到225W或者更高也能很好地应付（1.0标准峰值仅到75W）。</p>
<p>
    3\新增“输入输出虚拟化”(IOV)技术，可以让多台虚拟机共享网卡等PCI设备。</p>
<p>
    4\PCI-E线缆子规范可让PCI设备通过标准化铜缆线接入计算机，而且每条线路的速度都能达到2.5Gbps，适用于为高端服务器加入多块网卡作为输入输出扩展模块等场合。</p>
<p>
    5\最后是代号“Geneseo”的长期规划。该技术与Intel、IBM等业界巨头合作开发，可让图形处理单元、加密处理单元等协处理器更好地与中央处理器机密相连。 </p>
<p>
  <img width="500" height="332" src="显卡10年_clip_image046.jpg"  /> </p>
<p>
  <strong>小身材大味道——RV670是ATI改变的开端</strong> </p>
<p>
  <strong>　　</strong>在经过R600的失败后，ATI再次背水一战，推出了采用最新55nm工艺制造的RV670。这一次的赌博ATI奇迹般地顺利，有史以来第一次以A11版本（就是说第一版样品）核心即可出货相关产品，同时凭借当时最先进的55nm工艺，把RV670的每平方毫米性能比提升至极致的水平，就连NVIDIA的CEO黄仁勋先生都在年初的投资者会议上提到：“以高成本的G92产品去对抗对手的RV670产品实在有点顾此失彼”，可见RV670核心对NVIDIA的影响甚大。 </p>
<p>
  <strong>2008</strong><strong>年：</strong> </p>
<p>
  <img width="473" height="418" src="显卡10年_clip_image047.jpg"  /> </p>
<p>
  <strong>不慌不忙而来的旗舰——GT200</strong> </p>
<p>
  <strong>　　</strong>G80为我们打开了DirectX10之门，而GT200系列则为GPU写下了新的篇章。先不论其3D性能如何强大，从产品定位上来看NVIDIA已经不把其当作一款显卡看待，反正GT200更为适合做游戏以外的事情，例如视频编码，例如科研计算等等。 </p>
<p>
  <img width="500" height="302" src="显卡10年_clip_image048.jpg" alt="http://www.techcn.com.cn/uploads/200910/1254625965aW4YiIvA.jpg" /> </p>
<p>
    陪同GT200配套发布的还有CUDA，CUDA是由NVIDIA推出的通用并行计算架构，该架构使GPU能够解决复杂的计算问题。  它包含了CUDA指令集架构（ISA）以及GPU内部的并行计算引擎。 开发人员现在可以使用C语言来为CUDA架构编写程序，C语言是应用最广泛的一种高级编程语言。所编写出的程序于是就可以在支持CUDA的GPU上以超高性能运行。随着显卡的发展，GPU越来越强大，而且GPU为显示图像做了优化。在计算上已经超越了通用的CPU。如此强大的芯片如果只是作为显卡就太浪费了，因此NVIDIA推出CUDA，让显卡可以用于图像计算以外的目的。 </p>
<p>
  <img width="535" height="500" src="显卡10年_clip_image049.jpg" alt="http://www.techcn.com.cn/uploads/200910/1254625965uL3FqYhz.jpg" /> </p>
<p>
  <strong>ATI</strong><strong>的赌博——研发重点放在中型核心上</strong> </p>
<p>
    基于这种产能和可扩展性的考虑，未来ATI将把每一代核心的开发重点放在中型级芯片上，然后采用单卡双芯的方式推出顶级显卡。拒绝以前所走的老路子：先造出一颗大型GPU，然后不断屏蔽一部分规格以对应不同市场。在确定把所有精力放在研究中型级芯片后，ATI可谓在每平方毫米性能比方面登峰造极。如果说RV670是ATI着力研究发中型核心的第一步，那么RV770就是ATI中型核心的首款结晶品！让Geforce9800GTX一夜暴降、让NV合作伙伴倒戈相向、让经销商纷纷转型等事件，反映出RadeonHD4800系列在这次研发方向转型的赌博中胜出了。 </p>
<p>
  <strong>结语：</strong> </p>
<p>
    受到全球经济大环境的影响，2009年到目前为止还未见两大芯片设计厂商拿出什么让人惊喜的产品，不过I未来一年，INTEL的Larrabee、NVIDIA与ATI的新品将会打乱现有市场格局，并使形势发生变化；随着苹果、微软重量级新操作系统的问世，GPU通用计算的开发和应用也将更加深入，进一步刺激显卡市场；CUDA、DirectX 11、OpenCL等新的编程模式会充分挖掘GPU的潜在性能，使之成为所有PC中强大、高效的协处理器。 </p>
<p>
    参考文献 </p>
<p>
    http://publish.it168.com/2009/0724/20090724006901.shtml</p>

</div>
</body>
</html>
